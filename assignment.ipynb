{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "num_pages = 20\n",
        "url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_'\n",
        "\n",
        "# Create a CSV file to store the data\n",
        "csv_filename = 'amazon_products.csv'\n",
        "csv_file = open(csv_filename, 'w', newline='', encoding='utf-8')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews'])\n",
        "\n",
        "for page_number in range(1, num_pages + 1):\n",
        "    response = requests.get(url + str(page_number))\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the container for each product listing\n",
        "    product_containers = soup.find_all('div', class_='s-result-item')\n",
        "\n",
        "    for container in product_containers:\n",
        "        # Extract the URL\n",
        "        product_url_element = container.find('a', class_='a-link-normal')\n",
        "        if product_url_element is not None:\n",
        "            product_url = 'https://www.amazon.in' + product_url_element.get('href')\n",
        "        else:\n",
        "            product_url = 'N/A'\n",
        "\n",
        "        # Extract the product name\n",
        "        product_name_element = container.find('span', class_='a-size-medium')\n",
        "        if product_name_element is not None:\n",
        "            product_name = product_name_element.text.strip()\n",
        "        else:\n",
        "            product_name = 'N/A'\n",
        "\n",
        "        # Extract the product price\n",
        "        try:\n",
        "            product_price = container.find('span', class_='a-offscreen').text.strip()\n",
        "        except AttributeError:\n",
        "            product_price = 'N/A'\n",
        "\n",
        "        # Extract the rating\n",
        "        rating_element = container.find('span', class_='a-icon-alt')\n",
        "        if rating_element is not None:\n",
        "            rating = rating_element.text.strip()\n",
        "        else:\n",
        "            rating = 'N/A'\n",
        "\n",
        "        # Extract the number of reviews\n",
        "        num_reviews_element = container.find('span', class_='a-size-base')\n",
        "        if num_reviews_element is not None:\n",
        "            num_reviews = num_reviews_element.text.strip()\n",
        "        else:\n",
        "            num_reviews = 'N/A'\n",
        "\n",
        "        # Write the extracted data to the CSV file\n",
        "        csv_writer.writerow([product_url, product_name, product_price, rating, num_reviews])\n",
        "\n",
        "# Close the CSV file\n",
        "csv_file.close()\n",
        "\n",
        "print('Scraping complete. Data saved to', csv_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rzK9eHbHlZF",
        "outputId": "b41aa9f6-2f2c-4fd6-d684-be0d247b5c20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Data saved to amazon_products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "num_pages = 20\n",
        "url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_'\n",
        "\n",
        "# Create a CSV file to store the data\n",
        "csv_filename = 'amazon_products.csv'\n",
        "csv_file = open(csv_filename, 'w', newline='', encoding='utf-8')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews'])\n",
        "\n",
        "for page_number in range(1, num_pages + 1):\n",
        "    response = requests.get(url + str(page_number))\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the container for each product listing\n",
        "    product_containers = soup.find_all('div', class_='s-result-item')\n",
        "\n",
        "    for container in product_containers:\n",
        "        # Extract the URL\n",
        "        product_url_element = container.find('a', class_='a-link-normal s-no-outline')\n",
        "        if product_url_element is not None:\n",
        "            product_url = 'https://www.amazon.in' + product_url_element.get('href')\n",
        "        else:\n",
        "            product_url = 'N/A'\n",
        "\n",
        "        # Extract the product name\n",
        "        product_name_element = container.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
        "        if product_name_element is not None:\n",
        "            product_name = product_name_element.text.strip()\n",
        "        else:\n",
        "            product_name = 'N/A'\n",
        "\n",
        "        # Extract the product price\n",
        "        product_price_element = container.find('span', class_='a-offscreen')\n",
        "        if product_price_element is not None:\n",
        "            product_price = product_price_element.text.strip()\n",
        "        else:\n",
        "            product_price = 'N/A'\n",
        "\n",
        "        # Extract the rating\n",
        "        rating_element = container.find('span', class_='a-icon-alt')\n",
        "        if rating_element is not None:\n",
        "            rating = rating_element.text.strip()\n",
        "        else:\n",
        "            rating = 'N/A'\n",
        "\n",
        "        # Extract the number of reviews\n",
        "        num_reviews_element = container.find('span', class_='a-size-base')\n",
        "        if num_reviews_element is not None:\n",
        "            num_reviews = num_reviews_element.text.strip()\n",
        "        else:\n",
        "            num_reviews = 'N/A'\n",
        "\n",
        "        # Write the extracted data to the CSV file\n",
        "        csv_writer.writerow([product_url, product_name, product_price, rating, num_reviews])\n",
        "\n",
        "# Close the CSV file\n",
        "csv_file.close()\n",
        "\n",
        "print('Scraping complete. Data saved to', csv_filename)\n"
      ],
      "metadata": {
        "id": "n_81SahAHlWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd14522-2668-4f73-b45b-38e56b6c8f40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Data saved to amazon_products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to scrape product details from a given URL\n",
        "def scrape_product_details(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "    for product in products:\n",
        "        product_url_element = product.find('a', {'class': 'a-link-normal s-no-outline'})\n",
        "        product_url = 'https://www.amazon.in' + product_url_element['href'] if product_url_element else 'N/A'\n",
        "\n",
        "        product_name_element = product.find('span', {'class': 'a-size-medium a-color-base a-text-normal'})\n",
        "        product_name = product_name_element.text.strip() if product_name_element else 'N/A'\n",
        "\n",
        "        price_element = product.find('span', {'class': 'a-offscreen'})\n",
        "        product_price = price_element.text.strip() if price_element else 'N/A'\n",
        "\n",
        "        rating_element = product.find('span', {'class': 'a-icon-alt'})\n",
        "        rating = rating_element.text.strip().split()[0] if rating_element else 'N/A'\n",
        "\n",
        "        review_element = product.find('span', {'class': 'a-size-base'})\n",
        "        num_reviews = review_element.text.strip().replace(',', '') if review_element else 'N/A'\n",
        "\n",
        "        # Write data to CSV file\n",
        "        with open('products.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([product_url, product_name, product_price, rating, num_reviews])\n",
        "\n",
        "# Main scraping function\n",
        "def scrape_products(num_pages):\n",
        "    base_url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_'\n",
        "    for page in range(1, num_pages + 1):\n",
        "        url = base_url + str(page)\n",
        "        print(\"Scraping page\", page)\n",
        "        scrape_product_details(url)\n",
        "\n",
        "# Create a new CSV file and write header row\n",
        "with open('products.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews'])\n",
        "\n",
        "# Scrape 20 pages of product listings\n",
        "scrape_products(20)\n",
        "\n",
        "print(\"Scraping completed. Data saved to products.csv.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ran67jZaKvt1",
        "outputId": "21442ceb-758f-4f9f-86d3-b85d7746ac07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1\n",
            "Scraping page 2\n",
            "Scraping page 3\n",
            "Scraping page 4\n",
            "Scraping page 5\n",
            "Scraping page 6\n",
            "Scraping page 7\n",
            "Scraping page 8\n",
            "Scraping page 9\n",
            "Scraping page 10\n",
            "Scraping page 11\n",
            "Scraping page 12\n",
            "Scraping page 13\n",
            "Scraping page 14\n",
            "Scraping page 15\n",
            "Scraping page 16\n",
            "Scraping page 17\n",
            "Scraping page 18\n",
            "Scraping page 19\n",
            "Scraping page 20\n",
            "Scraping completed. Data saved to products.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Part 1: Scrape product data from the initial URL\n",
        "num_pages = 35\n",
        "url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_'\n",
        "\n",
        "# Create a CSV file to store the data\n",
        "csv_filename = 'amazon_products.csv'\n",
        "csv_file = open(csv_filename, 'w', newline='', encoding='utf-8')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews', 'Description', 'ASIN', 'Product Description', 'Manufacturer'])\n",
        "\n",
        "for page_number in range(1, num_pages + 1):\n",
        "    response = requests.get(url + str(page_number))\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the container for each product listing\n",
        "    product_containers = soup.find_all('div', class_='s-result-item')\n",
        "\n",
        "    for container in product_containers:\n",
        "        # Extract the URL\n",
        "        product_url_element = container.find('a', class_='a-link-normal s-no-outline')\n",
        "        if product_url_element is not None:\n",
        "            product_url = 'https://www.amazon.in' + product_url_element.get('href')\n",
        "        else:\n",
        "            product_url = 'N/A'\n",
        "\n",
        "        # Extract the product name\n",
        "        product_name_element = container.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
        "        if product_name_element is not None:\n",
        "            product_name = product_name_element.text.strip()\n",
        "        else:\n",
        "            product_name = 'N/A'\n",
        "\n",
        "        # Extract the product price\n",
        "        product_price_element = container.find('span', class_='a-offscreen')\n",
        "        if product_price_element is not None:\n",
        "            product_price = product_price_element.text.strip()\n",
        "        else:\n",
        "            product_price = 'N/A'\n",
        "\n",
        "        # Extract the rating\n",
        "        rating_element = container.find('span', class_='a-icon-alt')\n",
        "        if rating_element is not None:\n",
        "            rating = rating_element.text.strip()\n",
        "        else:\n",
        "            rating = 'N/A'\n",
        "\n",
        "        # Extract the number of reviews\n",
        "        num_reviews_element = container.find('span', class_='a-size-base')\n",
        "        if num_reviews_element is not None:\n",
        "            num_reviews = num_reviews_element.text.strip()\n",
        "        else:\n",
        "            num_reviews = 'N/A'\n",
        "\n",
        "        # Part 2: Visit each product URL to fetch additional details\n",
        "        if product_url != 'N/A':\n",
        "            product_response = requests.get(product_url)\n",
        "            product_soup = BeautifulSoup(product_response.text, 'html.parser')\n",
        "\n",
        "            # Extract the description, ASIN, product description, and manufacturer\n",
        "            description_element = product_soup.find('span', id='productTitle')\n",
        "            description = description_element.text.strip() if description_element is not None else 'N/A'\n",
        "\n",
        "            asin_element = product_soup.find('th', text='ASIN')\n",
        "            asin = asin_element.find_next('td').text.strip() if asin_element is not None else 'N/A'\n",
        "\n",
        "            product_desc_element = product_soup.find('div', id='productDescription')\n",
        "            product_desc = product_desc_element.text.strip() if product_desc_element is not None else 'N/A'\n",
        "\n",
        "            manufacturer_element = product_soup.find('a', id='bylineInfo')\n",
        "            manufacturer = manufacturer_element.text.strip() if manufacturer_element is not None else 'N/A'\n",
        "\n",
        "        else:\n",
        "            description = 'N/A'\n",
        "            asin = 'N/A'\n",
        "            product_desc = 'N/A'\n",
        "            manufacturer = 'N/A'\n",
        "\n",
        "        # Write the extracted data to the CSV file\n",
        "        csv_writer.writerow([product_url, product_name, product_price, rating, num_reviews, description, asin, product_desc, manufacturer])\n",
        "\n",
        "# Close the CSV file\n",
        "csv_file.close()\n",
        "\n",
        "print('Scraping complete. Data saved to', csv_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbkux36FLYLP",
        "outputId": "5e05d3c0-ac18-41b3-9df2-1bab97767809"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-8ee60a378e73>:67: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  asin_element = product_soup.find('th', text='ASIN')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Data saved to amazon_products.csv\n"
          ]
        }
      ]
    }
  ]
}